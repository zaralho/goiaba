<!DOCTYPE html>
<html lang="pt-br">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Página Inicial - Computação</title>
<link rel="stylesheet" href="estilo1.css">
<style>
p, h2, li, iframe {
  margin-left: 200px; margin-right: 200px; text-align: justify;
}

p, li {
  font-size: large;
}

body {
  animation: fadeInAnimation cubic-bezier(.45, 1, .3, 1) 3s;
  animation-iteration-count: 1;
  animation-fill-mode: forwards;
}
@keyframes fadeInAnimation {
  0% {
    opacity: 0;
  }
  
  100% {
    opacity: 1;
  }
}
  </style>
</head>
<header>
<h1 style="margin: 0.5%;" class="tit1">História da Computação</h1>
<div class="tit2">
<a style="color: #d4be5e;" href="index.html"><strong>Página inicial/Linha do tempo</strong></a> |
<a href="S2.html"><strong>Imagens/Vídeos/Filmes</strong></a> |
<a href="S3.html"><strong>Teste de Turing</strong></a>
</div>
</header>
<body>
  <h2>Descrição da linha do tempo</h2>
<p>O nosso ponto inicial, de acordo com as nossas informações atuais, começa em 3.000 a. C
 na Mesopotâmia com o &quot;Ábaco&quot;, um dispositivo simples de cálculo, que facilitava a
 contagem de produtos e gado na região,esse dispositivo é considerado por muitos o
 fundador da computação por permitir operações aritméticas rudimentares. Após milhares de
 anos, em 1614, um físico e matemático escocês, chamado de John Napier, de 63 anos,
 havia criado algo revolucionário nos subúrbios de londres, a “Régua de Cálculo” ,
 instrumento que desenvolveu a capacidade de realizar cálculos complexos através de
 escalas logarítmicas. No início da era vitoriana, 1837, Charles Babbage apresenta a
 &quot;Maquina analitica de Babbage&quot; ao mundo, algo que futuramente seria considerado um dos
 primeiros projetos de computador programável, mas infelizmente essa ideia nunca foi
 desenvolvida por completo.</p>
<p>No ano de 1941, Konrad Zuse, de 31 anos, desenvolve na Alemanha o &quot; Z3&quot;, mais
 conhecido como o primeiro computador funcional programável usando apenas
 eletromagnéticos. Na metade do ano de 1944, o mais famoso &quot;computador&quot; é lançado com
 o nome de &quot;Colossus&quot;,seu criador Alan Turing e sua equipe em Bletchley Park, construíram
 ele para decifrar códigos alemães durante a Segunda Guerra Mundial,hoje em dia ele é
 considerado um precursor dos computadores modernos. No ano seguinte tivemos o
 &quot;ENIAC&quot;, desenvolvido nos EUA, e considerado a primeira jamanta eletrônica utilizado
 inicialmente para cálculos balísticos.</p>
<p>Em 1951, ano da copa no Rio de Janeiro, a tecnologia começou a ser adotada pelas
 empresas e governos, mais especificamente um computador composto de logiciais
 produzidos por transístor, memória de núcleos magnéticos e batch remoto. 7 anos depois,
 em 1958, o ‘Circuito Integrado’,também chamado chip, microchip ou nano chip, foi integrado
 por Jack Kilby e Robert Noyce permitiu a miniaturização dos componentes eletrônicos,
 catalisando a revolução dos computadores pessoais e dispositivos eletrônicos modernos.
 Avançando um pouco mais, chegamos no ano de 1971, o mesmo ano da guerra de
 Bangladesh, e o ano em que a Intel lança o primeiro microprocessador comercial, o Intel
 4004, uma Unidade Central de Processamento com 4-bits, esse chip inaugurou a era dos
 computadores pessoais e da eletrônica embarcada.</p>
<p>Na década de 80, mais especificamente em 1983, A ARPANET precursora da internet
 moderna, é desenvolvida pelo DARPA nos Estados Unidos, conectando inicialmente
  universidades e instituições de pesquisa. Nesse mesmo ano ela trocou o protocolo NCP
 pelo novo TCP/IP. Em 1990, o ano em que ocorreu a clonagem da ovelha Dolly e o
 lançamento de ‘Esqueceram de mim’ (com o monstrinho do Kevin McCallister), o World
 Wide Web de Tim Berners-Lee, introduziu um sistema de hipertexto que facilita o acesso e
 a navegação de informações na internet.</p>
<p>No século seguinte, mais especificamente no ano 2000, a computação em nuvem é
 estrelada ao mundo, essa computação populariza, oferecendo serviços de armazenamento
 e processamento distribuídos pela internet, transformando a maneira como empresas e
 indivíduos utilizam recursos computacionais. Dez anos depois veio a inteligência artificial,
 que trouxe diversos avanços significativos em algoritmos de aprendizado de máquina e
 redes neurais revigoram o interesse e o desenvolvimento da inteligência artificial, com
 aplicações que vão desde assistentes virtuais até veículos autônomos.</p>
 </body>
</html>
